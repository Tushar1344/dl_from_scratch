{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tushar1344/dl_from_scratch/blob/main/Gpt2_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken >> //dev/null\n",
        "!pip install matplotlib >> //dev/null"
      ],
      "metadata": {
        "id": "HgqVicPCbA9t"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read in GPT2 using Huggingface"
      ],
      "metadata": {
        "id": "WLotgtfBaBBv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IJyvAFHofKfH"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "# sd_hf = model_hf.state_dict()\n",
        "\n",
        "# for k,v in sd_hf.items():\n",
        "#   print(k,v.shape)"
      ],
      "metadata": {
        "id": "K2Gt3ZS1iAwT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "# plt.imshow(sd_hf[\"transformer.wpe.weight\"])"
      ],
      "metadata": {
        "id": "X7xfRH0zjMLX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 150])\n",
        "#plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 200])\n",
        "#plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 250])"
      ],
      "metadata": {
        "id": "rDTz2b_MlyQL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(sd_hf[\"transformer.h.1.attn.c_attn.weight\"][:300, :300],cmap = \"gray\")"
      ],
      "metadata": {
        "id": "Y3xvYVOIhNTi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline, set_seed\n",
        "\n",
        "# generator = pipeline('text-generation', model='gpt2')\n",
        "# set_seed(42)\n",
        "# generator(\"Hello, I'm a language model\", max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "id": "H-YVC-amki6D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building your own GPT2"
      ],
      "metadata": {
        "id": "bpOwuxSEaLom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass  #for building the config object\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "pOgbr0-VlqGw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "  block_size: int = 1024 #Context length\n",
        "  vocab_size: int = 50257 #Number of tokens\n",
        "  n_layer: int = 12 #Number of layers\n",
        "  n_head: int = 12  #Number of heads\n",
        "  n_embd: int = 768 #Embedding size\n"
      ],
      "metadata": {
        "id": "ylzzsptYmcSR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  '''\n",
        "  A vanilla multi-head masked self-attention layer with a projection at the end\n",
        "  '''\n",
        "  def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # Key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # Regularization not present\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        # Not really a \"bias\", more of a mask, but following the OpenAI/HF naming though\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                             .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "        B, T, C = x.size()  # Batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        # Causal Self-Attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v  # (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # Re-assemble all head outputs side by side\n",
        "        y = self.c_proj(y)\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "GUGDXuTz36Eu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csa_obj = CausalSelfAttention(GPTConfig())"
      ],
      "metadata": {
        "id": "J6_ZDJywyex9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  '''MLP layer with GELU activation'''\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "    self.gelu = nn.GELU(approximate= 'tanh')\n",
        "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.c_fc(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.c_proj(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "y-sQmgcE2my2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_obj = MLP(GPTConfig())"
      ],
      "metadata": {
        "id": "MbyaqZbgztvq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  '''Transformer block: communication followed by computation'''\n",
        "  def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "      self.attn = CausalSelfAttention(config)\n",
        "      self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "      self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "XQ1AQvosxVzk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_obj = Block(GPTConfig())"
      ],
      "metadata": {
        "id": "QiCQL6PwyYI9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2 loading class"
      ],
      "metadata": {
        "id": "HAbUv4GzcwY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#class for loading gpt2 weights without having to use the huggingface library\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "      # idx is of shape (B, T)\n",
        "      B, T = idx.size()\n",
        "      assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is {self.config.block_size}\"\n",
        "\n",
        "      # Forward the token and position embeddings\n",
        "      pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # shape (T)\n",
        "      pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (T, n_embd)\n",
        "      tok_emb = self.transformer.wte(idx)  # token embeddings of shape (B, T, n_embd)\n",
        "      x = tok_emb + pos_emb\n",
        "\n",
        "      # Forward the blocks of the transformer\n",
        "      for block in self.transformer.h:\n",
        "          x = block(x)\n",
        "\n",
        "      # Forward the final layernorm and the classifier\n",
        "      x = self.transformer.ln_f(x)\n",
        "      logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "      loss = None\n",
        "      if targets is not None:\n",
        "          loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "\n",
        "      return logits, loss\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"\n",
        "        Loads pretrained GPT-2 model weights from Huggingface\n",
        "        \"\"\"\n",
        "        assert model_type in ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
        "\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = dict(\n",
        "            gpt2=dict(n_layer=12, n_head=12, n_embd=768),   # 124M params\n",
        "            gpt2_medium=dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            gpt2_large=dict(n_layer=36, n_head=20, n_embd=1280),  # 774M params\n",
        "            gpt2_xl=dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        )[model_type]\n",
        "\n",
        "        config_args['vocab_size'] = 50257  # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024  # always 1024 for GPT model checkpoints\n",
        "\n",
        "        # Create a from-scratch initialized miniGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "\n",
        "        # Load state_dict from pretrained model\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]  # discard this mask / buffer\n",
        "\n",
        "        # Initialize a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # Copy while ensuring all the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  # ignore these\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]  # same, just the mask\n",
        "\n",
        "        # Transposed keys for special handling\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "\n",
        "        assert len(sd_keys_hf) == len(sd_keys), \"Mismatched keys: {} vs {}\".format(len(sd_keys_hf), len(sd_keys))\n",
        "\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # Special treatment for the Conv1D weights that need to be transposed\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # Vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        model.load_state_dict(sd)\n",
        "        return model\n",
        "\n",
        "# Block and GPTConfig would need to be defined elsewhere in the code\n"
      ],
      "metadata": {
        "id": "4QjTuhJ2oJ00"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = GPT.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "jjFB0uqw7ZrR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #tiny shakespeare dataset\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "# with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()\n",
        "# data = text[:1000]\n",
        "# print(data[:100])"
      ],
      "metadata": {
        "id": "ROpRdqKrdbRP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wc input.txt"
      ],
      "metadata": {
        "id": "us9LxnmrkJqI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Train.py"
      ],
      "metadata": {
        "id": "Wr3jsoWTcmyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tb\n",
        "#Train.py code\n",
        "#attempt to autodetect the device\n",
        "import torch\n",
        "import tiktoken\n",
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "  device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "#device =\"cpu\" #OVERRIDE\n",
        "\n",
        "#tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "data = text[:1000]\n",
        "print(data[:100])\n",
        "\n",
        "#get a data batch\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "text = text[:1000]\n",
        "tokens = enc.encode(text)\n",
        "B, T = 4, 32\n",
        "buf = torch.tensor(tokens[:B*T+1])\n",
        "buf = buf.to(device)\n",
        "x = buf[:-1].view(B, T)\n",
        "y=buf[1:].view(B, T)\n",
        "\n",
        "#get logits\n",
        "config = GPTConfig()\n",
        "model = GPT(config)\n",
        "model.to(device)\n",
        "#x = x.to(device)\n",
        "#y = y.to(device)\n",
        "#logits, loss = model(x, y)\n",
        "\n",
        "#optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "for i in range(50):\n",
        "  logits, loss = model(x, y)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"step{i}, loss: {loss.item()}\")\n",
        "#print(logits.shape)\n",
        "#print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVIEuSCecISA",
        "outputId": "1cd3a90d-46fb-4dbe-b2e9-0e8cff5e397e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "--2024-09-09 02:57:30--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "\rinput.txt.2           0%[                    ]       0  --.-KB/s               \rinput.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-09-09 02:57:30 (107 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No traceback available to show.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "step0, loss: 10.97608470916748\n",
            "step1, loss: 6.763216495513916\n",
            "step2, loss: 6.228250980377197\n",
            "step3, loss: 3.105747938156128\n",
            "step4, loss: 1.9845982789993286\n",
            "step5, loss: 1.1414642333984375\n",
            "step6, loss: 0.6535788774490356\n",
            "step7, loss: 0.40266725420951843\n",
            "step8, loss: 0.2823127210140228\n",
            "step9, loss: 0.1855582892894745\n",
            "step10, loss: 0.12641243636608124\n",
            "step11, loss: 0.09013315290212631\n",
            "step12, loss: 0.06664373725652695\n",
            "step13, loss: 0.05083441734313965\n",
            "step14, loss: 0.04002544283866882\n",
            "step15, loss: 0.03346583992242813\n",
            "step16, loss: 0.027835454791784286\n",
            "step17, loss: 0.023455502465367317\n",
            "step18, loss: 0.019874952733516693\n",
            "step19, loss: 0.01707400009036064\n",
            "step20, loss: 0.014949685893952847\n",
            "step21, loss: 0.013048483058810234\n",
            "step22, loss: 0.011447412893176079\n",
            "step23, loss: 0.010210562497377396\n",
            "step24, loss: 0.009247585199773312\n",
            "step25, loss: 0.00845411978662014\n",
            "step26, loss: 0.007762356661260128\n",
            "step27, loss: 0.007142386399209499\n",
            "step28, loss: 0.006587280426174402\n",
            "step29, loss: 0.006097041070461273\n",
            "step30, loss: 0.005669317673891783\n",
            "step31, loss: 0.005297661758959293\n",
            "step32, loss: 0.004972231574356556\n",
            "step33, loss: 0.004681902937591076\n",
            "step34, loss: 0.0044181374832987785\n",
            "step35, loss: 0.004177996423095465\n",
            "step36, loss: 0.003961720038205385\n",
            "step37, loss: 0.003769019152969122\n",
            "step38, loss: 0.0035977386869490147\n",
            "step39, loss: 0.0034449738450348377\n",
            "step40, loss: 0.0033077257685363293\n",
            "step41, loss: 0.0031833620741963387\n",
            "step42, loss: 0.0030697910115122795\n",
            "step43, loss: 0.0029654381796717644\n",
            "step44, loss: 0.002869232324883342\n",
            "step45, loss: 0.0027802514377981424\n",
            "step46, loss: 0.0026980354450643063\n",
            "step47, loss: 0.0026220083236694336\n",
            "step48, loss: 0.0025517886970192194\n",
            "step49, loss: 0.0024869213812053204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "class DataloaderLite:\n",
        "  def __init__(self, B, T):\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "\n",
        "    #at init load tokens from disk and store them in memory\n",
        "    with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "      text = f.read()\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    tokens = enc.encode(text)\n",
        "    self.tokens = torch.tensors(tokens)\n",
        "    print(f\"loaded {len(tokens)} tokens\")\n",
        "    print(f\"1 epoch of data is {len(tokens)//(B*T)} batches\")\n",
        "\n",
        "    #state\n",
        "    self.current_position = 0\n",
        "\n",
        "  def next_batch(self):\n",
        "    B, T = self.B, self.T\n",
        "    buf = self.tokens[self.current_position:self.current_position+B*T+1]\n",
        "    x = buf[:-1].view(B, T) #inputs\n",
        "    y=buf[1:].view(B, T) #targets\n",
        "    #advance position in the tensor\n",
        "    self.current_position += B * T\n",
        "    #if loading the next batch will be OOB, then reset\n",
        "    if self.current_position + B * T >= len(self.tokens):\n",
        "      self.current_position = 0\n",
        "    return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "Yl5rKK5UmCpu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "\n",
        "#model= GPT.from_pretrained('gpt2')\n",
        "model= GPT(GPTConfig())\n",
        "model.eval()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GjudeUt892x",
        "outputId": "1f2e98e4-a38d-47da-84da-4aa3e85167a3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='tanh')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(data)\n",
        "print(tokens[:24])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwYH6QWInKgi",
        "outputId": "382dd912-5ebc-41a3-b125-7ec020803a8d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating labels from tokens for GPT2\n",
        "import torch\n",
        "buf = torch.tensor(tokens[:24+1])\n",
        "x = buf[:-1].view(4,6)\n",
        "y=buf[1:].view(4,6)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuWd1MXsneq6",
        "outputId": "987549df-4087-4b01-c5ac-8fbb0dc51303"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
            "        [ 5120,   597,  2252,    11,  3285,   502],\n",
            "        [ 2740,    13,   198,   198,  3237,    25],\n",
            "        [  198,  5248,   461,    11,  2740,    13]])\n",
            "tensor([[22307,    25,   198,  8421,   356,  5120],\n",
            "        [  597,  2252,    11,  3285,   502,  2740],\n",
            "        [   13,   198,   198,  3237,    25,   198],\n",
            "        [ 5248,   461,    11,  2740,    13,   198]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prefix tokens\n",
        "\n",
        "#gpt roughly has a 3:1 compression ratio for characters to tokens\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"Hello, I'm a language model\", allowed_special={\"<|endoftext|>\"})\n",
        "tokens = torch.tensor(tokens, dtype=torch.long, device='cuda')\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "x = tokens.to(device)"
      ],
      "metadata": {
        "id": "M_RFceAJJVzD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XwFJl4lnKh2V"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}